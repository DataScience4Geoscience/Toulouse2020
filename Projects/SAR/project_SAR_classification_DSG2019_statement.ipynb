{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAR IMAGE CLASSIFICATION\n",
    "- Classification problem\n",
    "- Oceanography/Meteorology\n",
    "- Pierre Tandeo (pierre.tandeo@imt-atlantique.fr), Ronan Fablet (ronan.fablet@imt-atlantique.fr)\n",
    "\n",
    "## Data and aim\n",
    "\n",
    "In this project, we use SAR (Synthetic Aperture Radar) images (20km x 20km) of the ocean surface. The goal is to automatically classify different oceanic and atmospheric phenomena. Below, we will find an example of 10 classes corresponding to 10 different phenomena (among pure ocean swell in F, convective cells in I, icebergs in L, etc...):\n",
    "\n",
    "![SAR](https://tandeo.files.wordpress.com/2019/01/sar_classes.png)\n",
    "\n",
    "Here, we will use a database of 160 SAR images per class, anotated by experts. Data were collected in 2016 by the Sentinel-1 satellite.\n",
    "\n",
    "## Evaluation and benchmark\n",
    "\n",
    "We divided the database in two parts: \"training\" and \"validation\" folders with respectively 70% and 30% of the dataset. To learn the model (here a classifier), we propose to use cross-validations on the training dataset. Then, you will use the validation dataset to evaluate the performance of your model, using the total pourcentage of well predicted data. The reference result is **94% of accuracy** and is given by a deep leaning architecture. The reference paper is given below.\n",
    "\n",
    "## References\n",
    "- https://arxiv.org/abs/1512.00567 (Inception-v3 paper)\n",
    "- https://tandeo.files.wordpress.com/2018/01/automated-geophysical-classification-of-sentinel-1-wave-mode-sar-images-through-deep-learning.pdf (classification of SAR images using Inception-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import classical libraries\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# figure size\n",
    "plt.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the datasets\n",
    "\n",
    "Now, we load the training (70%) and validation (30%) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.misc import imread\n",
    "\n",
    "# paths to the training and validation datasets\n",
    "path_train = \"/home/ptandeo/Dropbox/Documents/Data/SAR/training/\"\n",
    "path_validation = \"/home/ptandeo/Dropbox/Documents/Data/SAR/validation/\"\n",
    "\n",
    "# class names\n",
    "classes = ['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O']\n",
    "\n",
    "# initialization\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_validation = []\n",
    "y_validation = []\n",
    "\n",
    "# loop on images\n",
    "for j in range(len(classes)):\n",
    "    path = path_train + classes[j]\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    \n",
    "    # training\n",
    "    for i in range(len(files)):\n",
    "        tmp = imread(path + '/' + files[i])\n",
    "        X_train.append(ravel(tmp[0:450,0:450:,0]))\n",
    "        y_train.append(classes[j])\n",
    "    \n",
    "    # validation\n",
    "    path = path_validation + classes[j]\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    for i in range(len(files)):\n",
    "        tmp = imread(path + '/' + files[i])\n",
    "        X_validation.append(ravel(tmp[0:450,0:450:,0]))\n",
    "        y_validation.append(classes[j])\n",
    "        \n",
    "# transform to array\n",
    "X_train = asarray(X_train)\n",
    "y_train = asarray(y_train)\n",
    "X_validation = asarray(X_validation)\n",
    "y_validation = asarray(y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Apply naive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2625\n"
     ]
    }
   ],
   "source": [
    "# import functions\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# apply k-nearest classification\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_validation)\n",
    "\n",
    "# compute average classifier score\n",
    "print('Accuracy: '+ str(accuracy_score(y_validation, y_predict)))\n",
    "\n",
    "# we are far from the 94% accuracy given a deep learning model!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
