{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSG 2020: Regression methods (course)\n",
    "\n",
    "## Pierre Tandeo, IMT-Atlantique (pierre.tandeo@imt-atlantique.fr)\n",
    "\n",
    "In this notebook, we use different regression methods (both linear and nonlinear) to fit simulated data. We use a famous python library for machine learning: scikit-learn (http://scikit-learn.org/stable/index.html). Visit this website as soon as you need more information about a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library and adjust python parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import classical libraries\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# figure size\n",
    "rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "We generate $y$ (response/output variable) and $x$ (covariate/input variable) using the following model:\n",
    "$y = \\sin(x) + \\epsilon$, with $\\epsilon ~ \\sim \\mathcal{N}\\left(0, (1/4)^2\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate x and y\n",
    "random.seed(10) # setting seed for reproducability\n",
    "n = 100 # number of sample\n",
    "x = array([linspace(0, 5, n)]).T\n",
    "y = sin(x) + random.normal(0,0.25,size=shape(x))\n",
    "\n",
    "# plot data\n",
    "plot(x, y, '.')\n",
    "plot(x, sin(x), '-k')\n",
    "ylim([-2, 2])\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)\n",
    "title('Raw data and true model', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression\n",
    "\n",
    "In the simple linear regression, we adjust the model $y = \\beta_0 + \\beta_1 x$ with $\\beta_0$ the intercept and $\\beta_1$ the slope. You use generic functions of scikit-learn for regression:\n",
    "- *LinearRegression()* to declare the linear regression model\n",
    "- *fit()* to adjust the model and estimate the $\\beta$ parameters\n",
    "- *predict()* to apply a fitted model to the input data $x$ and get previsions $\\widehat{y}$\n",
    "- *intercept_* and *coef_* attributes to get the estimated $\\widehat{\\beta}$ parameters\n",
    "\n",
    "Then, we suggest to compute a score to evaluate the fitting performances. A famous score is the Root Mean Squared Error (RMSE) given by: $\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\widehat{y}_i \\right)^2}$ with $\\left( \\widehat{y}_1, \\dots, \\widehat{y}_n \\right)$ the predicted values.\n",
    "\n",
    "**Questions:**\n",
    "- adjust the simple linear regression \n",
    "- plot the fitted model\n",
    "- compute the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# adjust simple linear regression (slm) between x and y\n",
    "reg_slm = LinearRegression()\n",
    "reg_slm.fit(x, y)\n",
    "\n",
    "# print beta parameters\n",
    "print('beta_0 = ' + str(reg_slm.intercept_))\n",
    "print('beta_1 = ' + str(reg_slm.coef_))\n",
    "\n",
    "# use the fitted model to get predictions on x\n",
    "y_slm = reg_slm.predict(x)\n",
    "\n",
    "# plot results\n",
    "plot(x, y, '.')\n",
    "plot(x, y_slm, '-r')\n",
    "ylim([-2, 2])\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)\n",
    "title('Simple linear regression', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute score\n",
    "RMSE_slm = sqrt(mean((y - y_slm)**2))\n",
    "print('RMSE(slm) = ' + str(RMSE_slm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "As shown previously, if we only use x and the linear regression, we are not able to fit correctly the data. Thus, we propose to generate new predictors using different powers of $x$ such as $x^1$, $x^2$, $\\dots$, $x^{15}$. They will be stock in the $X$ matrix. Then, we will apply the linear regression between $X$ and $y$. This is known as the multiple linear regression (here, polynomial regression). The model is given by $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_{15} x^{15}$.\n",
    "\n",
    "**Questions:**\n",
    "- adjust the multiple linear regression and compute the RMSE\n",
    "- compare the results to the simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate X matrix\n",
    "X=zeros((n, 15))\n",
    "for i in range(15):\n",
    "    X[:,i] = ravel(x**(i+1))\n",
    "    \n",
    "#Â adjust multiple linear regression (mlr) between X and y\n",
    "reg_mlr = LinearRegression()\n",
    "reg_mlr.fit(X, y)\n",
    "y_mlr = reg_mlr.predict(X)\n",
    "\n",
    "# compute score\n",
    "RMSE_mlr = sqrt(mean((y - y_mlr)**2))\n",
    "print('RMSE(mlr) = ' + str(RMSE_mlr))\n",
    "\n",
    "# plot results\n",
    "plot(x, y, '.')\n",
    "plot(x, y_mlr, '-r')\n",
    "ylim([-2, 2])\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)\n",
    "title('Multiple linear regression', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation to choose the best number of predictors\n",
    "\n",
    "In the previous examples, we have seen that if we add predictors, we reduce the RMSE score. But, as we use the same datasets to fit and test the model, we are maybe overfitting the data... To test this and evaluate the optimal number of predictors in the multiple linear regression, we propose to use the following cross-validation procedure:\n",
    "- divide the dataset randomly (2/3 for training and 1/3 for test) for each iteration\n",
    "- use 1000 iterations\n",
    "- compute the median RMSE of the 1000 iterations\n",
    "\n",
    "This procedure is easy to implement in scikit-learn using *ShuffleSplit()* and *cross_val_score()*.\n",
    "\n",
    "**Questions:**\n",
    "- declare the cross-validation procedure\n",
    "- calculate the RMSE for each degree of the polynomial by cross-validation\n",
    "- apply the regression with the optimal number of degrees for the polynomial regression\n",
    "- plot and compare this fit to the true model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "# declare the cross-validation procedure (cvp)\n",
    "cvp = ShuffleSplit(n_splits=1000, test_size=1/3, train_size=2/3)\n",
    "\n",
    "# example of computed RMSE on 100 cross-validations using multiple linear regression\n",
    "RMSE = sqrt(-cross_val_score(reg_mlr, X, y, scoring='neg_mean_squared_error', cv=cvp))\n",
    "\n",
    "# define the number of degrees between 1 and 15\n",
    "n_degrees = 15\n",
    "degrees = range(1, 16)\n",
    "\n",
    "# loop on number of predictors and compute mean RMSE\n",
    "tab_RMSE_mlr = zeros(n_degrees)\n",
    "for i in range(n_degrees):\n",
    "    tab_RMSE_mlr[i] = mean(sqrt(-cross_val_score(reg_mlr, X[:,0:degrees[i]], y, scoring='neg_mean_squared_error', cv=cvp)))\n",
    "\n",
    "# plot results\n",
    "plot(degrees, tab_RMSE_mlr)\n",
    "xlabel('Degree of the polynomial', size=20)\n",
    "ylabel('RMSE', size=20)\n",
    "title('Cross-validation', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply polynomial regression with 3 degrees\n",
    "reg_mlr = LinearRegression()\n",
    "reg_mlr.fit(X[:,0:3], y)\n",
    "y_mlr = reg_mlr.predict(X[:,0:3])\n",
    "\n",
    "# plot results\n",
    "plot(x, y, '.')\n",
    "line1, = plot(x, y_mlr, '-r')\n",
    "line2, = plot(x, sin(x), '-k')\n",
    "ylim([-2, 2])\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)\n",
    "legend([line1, line2], ['3 degrees polynomial', 'true model'], prop={'size': 20})\n",
    "title('Multiple linear regression', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lasso regression\n",
    "\n",
    "Another way to select the optimal model is to use the Lasso regression. Lasso adds a sparse constraint on the $\\beta$ parameters and try to find the $\\beta$ parameters that minimize: $\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{i,j} \\right)^2 + \\alpha\\sum_{j=1}^p \\left| \\beta_j \\right|$.\n",
    "\n",
    "**Questions:**\n",
    "- generate $\\alpha = (10^{-5}, \\dots , 1)$\n",
    "- for each alpha, compute the median RMSE by cross-validation\n",
    "- for the best alpha parameter, have a look at the estimated $\\beta$ parameters\n",
    "- plot and compare this fit to the true model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# define the alphas between 10^(-5) and 1\n",
    "n_alphas = 10\n",
    "alphas = logspace(-5, 0, n_alphas)\n",
    "\n",
    "# loop on the alpha parameters and compute mean RMSE\n",
    "tab_RMSE_lasso = zeros(n_alphas)\n",
    "for i in range(n_alphas):\n",
    "    reg_lasso = Lasso(alphas[i])\n",
    "    tab_RMSE_lasso[i] = mean(sqrt(-cross_val_score(reg_lasso, X, y, scoring='neg_mean_squared_error', cv=cvp)))\n",
    "\n",
    "# plot results\n",
    "plot(alphas, tab_RMSE_lasso)\n",
    "xscale('log')\n",
    "xlabel('Alpha coefficients (lasso)', size=20)\n",
    "ylabel('RMSE', size=20)\n",
    "title('Cross-validation', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adjust lasso regression between X and y\n",
    "reg_lasso = Lasso(alpha=0.001)\n",
    "reg_lasso.fit(X, y)\n",
    "y_lasso = reg_lasso.predict(X)\n",
    "\n",
    "# print beta parameters\n",
    "print('beta: ' + str(reg_lasso.coef_))\n",
    "\n",
    "# plot results\n",
    "plot(x, y, '.')\n",
    "line1, = plot(x, y_lasso, '-r')\n",
    "line2, = plot(x, sin(x), '-k')\n",
    "ylim([-2, 2])\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)\n",
    "title('Lasso regression', size=20)\n",
    "legend([line1, line2], ['lasso regression', 'true model'], prop={'size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Local linear regression\n",
    "\n",
    "Now, we propose to use a nonlinear regression method. In this case, we will **not use anymore X** (the matrix with the 15 degrees of polynomial) but we will **only use x** (the initial input vector).\n",
    "\n",
    "An efficient method consists in adjusting, for each new valuation point x*, a local linear regression using the k-nearest neighbors. In practice, this regression is robust when the number or input variables $x_1,\\dots,x_p$ is important.\n",
    "\n",
    "The local linear regression method is not provided by scikit-learn: we code it here.\n",
    "\n",
    "**Questions:**\n",
    "- code the local linear regression\n",
    "- fit the local linear regression with $\\lambda=0.2$\n",
    "- plot and compare this fit to the true model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to compute weights using a Gaussian kernel\n",
    "def compute_weights(x_star, x, lmbda):\n",
    "    \n",
    "    # apply the Gaussian kernel\n",
    "    w = exp((-(x_star-x)**2)/lmbda)\n",
    "    \n",
    "    # normalize the weights\n",
    "    w = w/sum(w)\n",
    "    \n",
    "    # return the weights\n",
    "    return w\n",
    "\n",
    "# local linear regression function\n",
    "def local_linear_regression(x, y, lmbda):\n",
    "    \n",
    "    # initialization\n",
    "    y_llr = y*NaN\n",
    "    \n",
    "    # loop on samples\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # compute the weights\n",
    "        w = compute_weights(x[i], x, lmbda)\n",
    "        \n",
    "        # apply the weighted regression\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(x, y, ravel(w))\n",
    "        y_llr[i] = reg.predict(array([x[i]]))\n",
    "        \n",
    "    # return the estimations\n",
    "    return y_llr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adjust local linear regression between x and y\n",
    "lmbda = 0.2\n",
    "y_llr = local_linear_regression(x, y, lmbda)\n",
    "\n",
    "# plot results\n",
    "plot(x, y, '.')\n",
    "line1, = plot(x, y_llr, '-r')\n",
    "line2, = plot(x, sin(x), '-k')\n",
    "ylim([-2, 2])\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)\n",
    "title('Local linear regression', size=20)\n",
    "legend([line1, line2], ['lambda='+str(lmbda), 'true model'], prop={'size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice on regression and chaotic model\n",
    "\n",
    "Now, you are ready to start the practice on regression. It is based on the Lorenz model, also called the strange attractor. It is given in *DSG_2020_regression_practice.ipynb*."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
