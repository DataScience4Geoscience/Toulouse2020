{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be run on colab: [here](https://colab.research.google.com/github/DataScience4Geoscience/Toulouse2020/blob/master/Notebooks/Classification/N4_Model_Selection_and_Assessment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification framework\n",
    "\n",
    "Two steps are required\n",
    "- Model selection, i.e. find the optimal hyperparemeters,\n",
    "- Model assessement, i.e. validate the model on unseen data.\n",
    "\n",
    "Scikit-learn offers convenient and generic functions to achieve these steps. In what follow, an example is given for SVM. But it can be extended for any algorithm in scikit-learn, up to a correct definition of the hyperparameters. In this labworks, we use the support vector classifier (SVC) (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) on the digit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_samples=1000, n_features=50, n_informative=40, n_redundant=10,\n",
    "                          n_classes=5, n_clusters_per_class=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With kernel methods (and more generaly for any distance based methods), it is a good practice to standardize feature remove dynamics effect. Here we rescale each feature between 0 and 1. This step can be done inside or outside the cross validation loop, since it is does not depende on the label, it does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X) # Scale data between 0 and 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebooks, three kernels will be investigated:\n",
    "- A linear kernel, in that case, there is only one hyperparameter to tune (regularization).\n",
    "- A polynomial kernel, in that case, there are two parameters to tune (regularization + order)\n",
    "- A Gaussian kernel, in that case, tere are two parameters to tune (regularization + scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = sp.arange(0,8) # Degree of the polynomial kernel\n",
    "C = 10.0**sp.arange(-1,3) # Penality of the optimization problem\n",
    "gamma = 2.0**sp.arange(-4,4) # Scale of the RBF kernel\n",
    "params = [dict(kernel=['linear'], C=C),\n",
    "          dict(kernel=['poly'],degree=degree, C=C),\n",
    "          dict(kernel=['rbf'],gamma=gamma, C=C)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to run all in once, but for illustrative purpose, we apply the CV for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, params_ in zip(['Linear', 'Poly', 'Gaussian'], params):\n",
    "    print(\"Classifier {}\".format(clf))\n",
    "    grid = GridSearchCV(SVC(),  # Set up the classifier\n",
    "                    param_grid=params_, \n",
    "                    cv= 5,\n",
    "                    n_jobs=-1) # Do the grid search in parallel\n",
    "    grid.fit(X_train, y_train) # Run the grid search\n",
    "\n",
    "    # Print some results\n",
    "    print(\"Best score: {}\".format(grid.best_score_)) # Default scorer in scikit is the correct classification rate\n",
    "    print(\"Best set of hyperparameters: {}\".format(grid.best_params_))\n",
    "    \n",
    "    # Learn the optimal model\n",
    "    clf = grid.best_estimator_  # Get the best estimator\n",
    "    clf.fit(X_train,y_train)  # Fit it using the training set\n",
    "    \n",
    "    # Predict new samples\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute the overall accuracies\n",
    "    print(\"Correct classification rate on the test set: {}\\n\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the correct classification rate function of the hyperparameters. It is important to check if our search values are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = grid.cv_results_['mean_test_score'].reshape(C.size,gamma.size)\n",
    "X, Y = sp.meshgrid(gamma, C)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "cp = ax.contourf(X, Y, res)\n",
    "ax.scatter(grid.best_params_['gamma'],grid.best_params_['C'],color='k')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Gamma\")\n",
    "ax.set_ylabel(\"C\")\n",
    "fig.colorbar(cp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
