{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Polyadic Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are going to perform CPD with tensor data.\n",
    "\n",
    "Canonical Polyadic Decomposition (CPD), also known as Parallel Factor Analysis (or PARAFAC), is a tensor decomposition technique that guarantees the diagonality of the core tensor. Consequently, it can be seen as a direct extension of matrix factorization techniques to high-order data. For that, CPD is widely used as the main decomposition technique to extract information.\n",
    "\n",
    "Definition: A rank-$1$ tensor of order $N$ is one that is created from an outer product of $N$ vectors. They are also called decomposable tensors.\n",
    "\n",
    "The CPD is very important since it reveals the rank of the tensor, which is defined as the least number of rank-$1$ tensors for the CPD to be exact. A tensor of rank $R$ for instance is then called a rank-$R$ tensor.\n",
    "\n",
    "Assuming a third-order tensor $\\boldsymbol{\\mathcal{T}}$ of dimensions $I \\times J\\times K$ and rank $R$, the CPD can be written in tucker form (also matrix form or compact form) as:\n",
    "$$\\boldsymbol{\\mathcal{T}} = \\boldsymbol{\\mathcal{D}} \\mathop{\\bullet}_1 \\textbf{A} \\mathop{\\bullet}_2 \\textbf{B} \\mathop{\\bullet}_3 \\textbf{C}$$\n",
    "\n",
    "Where:\n",
    "+ $\\textbf{A}\\in\\mathbb{R}^{I\\times R}$, the first factor matrix, is related to the first mode of $\\boldsymbol{\\mathcal{T}}$.\n",
    "+ $\\textbf{B}\\in\\mathbb{R}^{J\\times R}$, the second factor matrix, is related to the second mode of $\\boldsymbol{\\mathcal{T}}$.\n",
    "+ $\\textbf{C}\\in\\mathbb{R}^{K\\times R}$, the third factor matrix, is related to the third mode of $\\boldsymbol{\\mathcal{T}}$.\n",
    "+ $\\boldsymbol{\\mathcal{D}}\\in\\mathbb{R}^{I\\times J\\times K}$, the core tensor, is square and diagonal. When the columns of the factor matrices are normalized, the norms are stored in the diagonal of $\\boldsymbol{\\mathcal{D}}$. If the factor matrices conserve their norms, then $\\boldsymbol{\\mathcal{D}}$ is an Identity tensor (i.e. $\\boldsymbol{\\mathcal{D}}_{r,r,r} = 1$ for $r=\\{1\\dots R\\}$ and $0$ otherwise).\n",
    "\n",
    "Due to the diagonality of the core tensor, the element-wise representation reduces to:\n",
    "$$t_{ijk} = \\sum_{r=1}^{R} d_{rrr} \\cdot a_{ir} \\cdot b_{jr} \\cdot c_{kr}$$\n",
    "\n",
    "which also contributes to the vector form of the CPD, written as the sum of the $R$ outer products of the columns of the factor matrices:\n",
    "$$\\boldsymbol{\\mathcal{T}} =  \\sum_{r=1}^{R} d_{r,r,r} \\cdot (a_{r} \\otimes b_{r} \\otimes c_{r})$$\n",
    "\n",
    "Computing the CPD of this tensor is equivalent to solving the cost function:\n",
    "$$\\textrm{arg}\\min_{\\textbf{A},\\textbf{B},\\textbf{C}}{ \\frac{1}{2} \\| \\boldsymbol{\\mathcal{T}} - (\\textbf{A} , \\textbf{B} , \\textbf{C}) \\cdot \\boldsymbol{\\mathcal{D}} \\|_F^2 }$$\n",
    "\n",
    "The latter cost function is another reason why CPD is interesting, which is that the decomposition can host a variety of constraints, most notably those of nonnegativity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tasks, we use the tensor library made available by TensorLy:\n",
    "+ Jean Kossaifi, Yannis Panagakis, Anima Anandkumar and Maja Pantic, TensorLy: Tensor Learning in Python, Journal of Machine Learning Research (JMLR), 2019, volume 20, number 26.\n",
    "\n",
    "First, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly import kruskal_tensor\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy.io import loadmat\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case of a randomly created tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tensor: (10, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create a random 2x3x4 tensor and see how the dimensions appear in the print\n",
    "T = np.random.random((10, 10, 10))\n",
    "\n",
    "print(\"Size of the tensor:\",T.shape)\n",
    "\n",
    "# print(\"\\nRandomly created tensor:\\n\",T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor Rank = 7 \n",
      "\n",
      "Norm of A = [1.000 1.000 1.000 1.000 1.000 1.000 1.000] \n",
      "\n",
      "Norm of B = [1.000 1.000 1.000 1.000 1.000 1.000 1.000] \n",
      "\n",
      "Norm of C = [1.000 1.000 1.000 1.000 1.000 1.000 1.000] \n",
      "\n",
      "Diagonal of core tensor D =  [16.357 2.437 6.207 2.597 6.305 2.334 2.099] \n",
      "\n",
      "Size of A = (10, 7)\n",
      "Size of B = (10, 7)\n",
      "Size of C = (10, 7)\n",
      "Size of Core Tensor D (as a vector) = (7,)\n"
     ]
    }
   ],
   "source": [
    "# Specify the multilinear ranks\n",
    "R = 7\n",
    "print(\"\\nTensor Rank =\",R,\"\\n\")\n",
    "\n",
    "# Calculate the factor matrices using the CPD\n",
    "# If ``normalize_factors`` is set to True, then the columns\n",
    "# of the factor matrices will be normalized and absorbed in\n",
    "# the core tensors. Otherwise, the core tensor will be an \n",
    "# identity tensor and the factor matrices are not normalized.\n",
    "factors = parafac(T , R , 100, normalize_factors=True)\n",
    "\n",
    "# ``factors`` is a special type variable defined as\n",
    "# KruskalTensor, consisting of the ``weights`` that represent\n",
    "# the diagonal elements but are set to 1, and the ``matrices``\n",
    "# that are the factor matrices of the decomposition:\n",
    "weights = factors[0]\n",
    "matrices = factors[1]\n",
    "\n",
    "# Factor Matrices and Core Tensor\n",
    "A = matrices[0]\n",
    "B = matrices[1]\n",
    "C = matrices[2]\n",
    "D = weights\n",
    "\n",
    "# Check the norms of the columns of the factor matrices\n",
    "norm_A = np.sqrt( np.sum( np.square(A),axis=0 ) )\n",
    "norm_B = np.sqrt( np.sum( np.square(B),axis=0 ) )\n",
    "norm_C = np.sqrt( np.sum( np.square(C),axis=0 ) )\n",
    "print(\"Norm of A =\", norm_A,\"\\n\")\n",
    "print(\"Norm of B =\", norm_B,\"\\n\")\n",
    "print(\"Norm of C =\", norm_C,\"\\n\")\n",
    "print(\"Diagonal of core tensor D = \", D,\"\\n\")\n",
    "\n",
    "print(\"Size of A =\", np.shape(A))\n",
    "print(\"Size of B =\", np.shape(B))\n",
    "print(\"Size of C =\", np.shape(C))\n",
    "print(\"Size of Core Tensor D (as a vector) =\", np.shape(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, it is good to observe the reconstruction error between $\\boldsymbol{\\mathcal{T}}$ and $\\boldsymbol{\\mathcal{\\hat{T}}}$ with respect to the rank.**\n",
    "+ Try to make a plot of how the reconstruction error progresses with respect to the different values of rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error using rank R = 7  is: 38.13%\n"
     ]
    }
   ],
   "source": [
    "# Construct the estimated tensor based on the estimated factors\n",
    "T_hat = tl.kruskal_to_tensor(factors)\n",
    "\n",
    "# Reconstruction error\n",
    "t = tl.tensor_to_vec(T)\n",
    "t_hat = tl.tensor_to_vec(T_hat)\n",
    "rec_err = la.norm(t-t_hat) / la.norm(t) * 100\n",
    "\n",
    "print(\"The reconstruction error using rank R =\", R ,\" is: \" + \"{:.2f}\".format(rec_err) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case of a tensor created from available factors:\n",
    "\n",
    "**A good practice at this point is to create a tensor starting from factor matrices of meaningful data, or load a meaningful tensor. After that, decompose the tensor and observe the differences in the decomposed factors.**\n",
    "\n",
    "+ Note that in the case where a tensor is created from factor matrices, the latters should have the same number of columns (corresponding to the rank). If the rank is small, it is worth checking if decomposing the tensor using the same rank returns the same factor matrices (by means of kruskal's condition for uniqueness of the CPD, Kruskal et al. 1977). Attention that CPD is unique up to scaling and permutation, i.e. the returned matrices can be exactly the same as those that were initialized, but their columns might be permuted.\n",
    "+ Note that each matrix corresponds to one mode of the tensor, so the matrices are assumed to hold related information when the modes of the tensor hold some meaning or describe a diversity.\n",
    "\n",
    "**Finally, The CPD can be thought of as a method to separate between the distinct modes (by means of the factor matrices) and re-formulate the complex multidimensional data into simple matrices with countable, pre-defined amount of components (by means of the chosen value of the rank).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of A = [1.000 1.000 1.000 1.000 1.000] \n",
      "\n",
      "Norm of B = [1.000 1.000 1.000 1.000 1.000] \n",
      "\n",
      "Norm of C = [1.000 1.000 1.000 1.000 1.000] \n",
      "\n",
      "Diagonal of core tensor D =  [40.603 44.039 41.113 72.555 49.088] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from factor matrices\n",
    "\n",
    "# Specify the dimensions of the factors\n",
    "R = 5 # rank\n",
    "I = 20 # dimension of first mode\n",
    "J = 15 # dimension of second mode\n",
    "K = 10 # dimension of third mode\n",
    "\n",
    "# Create the factor matrices\n",
    "A = np.random.randn(I,R)\n",
    "B = np.random.randn(J,R)\n",
    "C = np.random.randn(K,R)\n",
    "\n",
    "# Normalize the factor matrices\n",
    "norm_A = np.sqrt( np.sum( np.square(A),axis=0 ) )\n",
    "norm_B = np.sqrt( np.sum( np.square(B),axis=0 ) )\n",
    "norm_C = np.sqrt( np.sum( np.square(C),axis=0 ) )\n",
    "\n",
    "A = A / norm_A\n",
    "B = B / norm_B\n",
    "C = C / norm_C\n",
    "\n",
    "# Store the norms in the core tensor\n",
    "D = norm_A * norm_B * norm_C\n",
    "\n",
    "# Update the norms to confirm the normalization\n",
    "norm_A = np.sqrt( np.sum( np.square(A),axis=0 ) )\n",
    "norm_B = np.sqrt( np.sum( np.square(B),axis=0 ) )\n",
    "norm_C = np.sqrt( np.sum( np.square(C),axis=0 ) )\n",
    "\n",
    "print(\"Norm of A =\", norm_A,\"\\n\")\n",
    "print(\"Norm of B =\", norm_B,\"\\n\")\n",
    "print(\"Norm of C =\", norm_C,\"\\n\")\n",
    "print(\"Diagonal of core tensor D = \", D,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the created tensor is: (20, 15, 10)\n",
      "Size of A_hat = (20, 5)\n",
      "Size of B_hat = (15, 5)\n",
      "Size of C_hat = (10, 5)\n",
      "Size of Core Tensor D_hat (as a vector) = (5,)\n",
      "Diagonal of core tensor D_hat =  [72.555 49.088 44.039 41.113 40.603] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a KruskalTensor instance by initializing it as [weights,[factor matrices]]\n",
    "factors = kruskal_tensor.KruskalTensor([D,[A,B,C]])\n",
    "\n",
    "# Create the tensor from factor matrices\n",
    "T = tl.kruskal_to_tensor(factors)\n",
    "print(\"The size of the created tensor is:\", np.shape(T))\n",
    "\n",
    "# Calculate the factor matrices using the CPD\n",
    "# If ``normalize_factors`` is set to True, then the columns\n",
    "# of the factor matrices will be normalized and absorbed in\n",
    "# the core tensors. Otherwise, the core tensor will be an \n",
    "# identity tensor and the factor matrices are not normalized.\n",
    "factors_hat = parafac(T , R , 100, normalize_factors=True)\n",
    "\n",
    "# ``factors`` is a special type variable defined as\n",
    "# KruskalTensor, consisting of the ``weights`` that represent\n",
    "# the diagonal elements but are set to 1, and the ``matrices``\n",
    "# that are the factor matrices of the decomposition:\n",
    "weights = factors_hat[0]\n",
    "matrices = factors_hat[1]\n",
    "\n",
    "# Factor Matrices and Core Tensor\n",
    "A_hat = matrices[0]\n",
    "B_hat = matrices[1]\n",
    "C_hat = matrices[2]\n",
    "D_hat = weights\n",
    "\n",
    "print(\"Size of A_hat =\", np.shape(A_hat))\n",
    "print(\"Size of B_hat =\", np.shape(B_hat))\n",
    "print(\"Size of C_hat =\", np.shape(C_hat))\n",
    "print(\"Size of Core Tensor D_hat (as a vector) =\", np.shape(D_hat))\n",
    "print(\"Diagonal of core tensor D_hat = \", D_hat,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the results:\n",
    "+ Compare the results between the original factors $\\textbf{A}$, $\\textbf{B}$, $\\textbf{C}$, $\\boldsymbol{\\mathcal{D}}$, and their estimated versions $\\hat{\\textbf{A}}$, $\\hat{\\textbf{B}}$, $\\hat{\\textbf{C}}$, $\\boldsymbol{\\mathcal{\\hat{D}}}$.\n",
    "+ How much is the reconstruction error considering we used the same value of the rank? What conclusion do we reach about the tensor when the reconstruction is perfect?\n",
    "+ Do you notice any permutation in the columns of the matrices?\n",
    "+ Finally, try changing the value of the rank and observe the differences in the factors and the reconstruction error. When the rank of the decomposition is chosen to be lower than the one used to reconstruct the tensor, what effect do you think it brings on the estimated factors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case of loading a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
